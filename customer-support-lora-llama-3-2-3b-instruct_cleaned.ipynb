{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TyQJSFWpA_6d"
   },
   "source": [
    "# Installing necessary libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eKCYFhbCBBJq"
   },
   "source": [
    "* transformers: Provides state-of-the-art pretrained models for NLP, computer vision, and beyond.\n",
    "* datasets: A library for easy access to a wide range of datasets for ML and NLP tasks.\n",
    "* accelerate: Simplifies distributed training and inference for PyTorch models.\n",
    "* torch: PyTorch library for building and training deep learning models.\n",
    "* bitsandbytes: Optimized GPU quantization and acceleration for large-scale models.\n",
    "* peft: Parameter-efficient fine-tuning techniques for large language models.\n",
    "* trl: Tools for training transformer models with reinforcement learning techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install transformers==4.47.1 accelerate==0.34.2 bitsandbytes==0.45.0 trl==0.13.0 datasets==3.2.0 peft==0.14.0 tokenizers==0.21.0 huggingface_hub==0.26.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -U bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install peft trl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install tokenizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TQVzyOuhBGPi"
   },
   "source": [
    "# Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T15:39:12.793818Z",
     "iopub.status.busy": "2025-04-21T15:39:12.793511Z",
     "iopub.status.idle": "2025-04-21T15:39:20.415693Z",
     "shell.execute_reply": "2025-04-21T15:39:20.415018Z",
     "shell.execute_reply.started": "2025-04-21T15:39:12.793793Z"
    },
    "id": "0AkMx3I9BGG-"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, LlamaTokenizer, BitsAndBytesConfig\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from peft import PeftModel,get_peft_model,LoraConfig, TaskType\n",
    "from trl import SFTTrainer, SFTConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T15:39:20.417006Z",
     "iopub.status.busy": "2025-04-21T15:39:20.416769Z",
     "iopub.status.idle": "2025-04-21T15:39:20.762020Z",
     "shell.execute_reply": "2025-04-21T15:39:20.761165Z",
     "shell.execute_reply.started": "2025-04-21T15:39:20.416984Z"
    },
    "id": "n9P7L6ZzBH4L"
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "user_secrets = UserSecretsClient()\n",
    "hf_token = user_secrets.get_secret(\"HuggingFace\") # Fetching the Hugging Face token from the Kaggle Secret keys add on\n",
    "login(token = hf_token) # Logging into Hugging Face Hub to access models and other resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XJg9kA5MBPDy"
   },
   "source": [
    "# Loading Model configurations and Dataset Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5WgqYkFrBgXL"
   },
   "source": [
    "Huggingface model link: https://huggingface.co/meta-llama/Llama-3.2-3B-Instruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T15:39:20.763917Z",
     "iopub.status.busy": "2025-04-21T15:39:20.763654Z",
     "iopub.status.idle": "2025-04-21T15:39:20.767233Z",
     "shell.execute_reply": "2025-04-21T15:39:20.766494Z",
     "shell.execute_reply.started": "2025-04-21T15:39:20.763896Z"
    },
    "id": "LBACybdSBOVi"
   },
   "outputs": [],
   "source": [
    "base_model = 'meta-llama/Llama-3.2-3B-Instruct'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T15:39:20.768306Z",
     "iopub.status.busy": "2025-04-21T15:39:20.768032Z",
     "iopub.status.idle": "2025-04-21T15:39:28.982242Z",
     "shell.execute_reply": "2025-04-21T15:39:28.981265Z",
     "shell.execute_reply.started": "2025-04-21T15:39:20.768283Z"
    },
    "id": "z7L4sqYLBn7z",
    "outputId": "c32ef5f7-55d4-49b5-e0bb-51db4f436cea"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ba75a5a8c124204964fc642abcac850",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Configure 4-bit quantization settings using the BitsAndBytesConfig class\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,  # Enable loading the model with 4-bit precision for reduced memory usage\n",
    "    bnb_4bit_quant_type='nf4',  # Use NormalFloat4 (nf4), a quantization format for higher accuracy\n",
    "    bnb_4bit_compute_dtype=torch.float16,  # Use float16 for computation to balance speed and precision\n",
    "    bnb_4bit_use_double_quant=True  # Enable double quantization for better numerical stability\n",
    ")\n",
    "\n",
    "# Load the pre-trained model with 4-bit quantization\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model,  # Name of the base model defined earlier\n",
    "    device_map=\"auto\",  # Automatically map model layers to available devices (e.g., GPU/CPU)\n",
    "    quantization_config=bnb_config,  # Apply the defined 4-bit quantization configuration\n",
    ")\n",
    "\n",
    "# Note:\n",
    "# 1. The use of 4-bit quantization helps in reducing memory requirements while maintaining reasonable performance.\n",
    "# 2. `device_map=\"auto\"` ensures the model layers are automatically distributed across available hardware for efficient loading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T15:39:28.984044Z",
     "iopub.status.busy": "2025-04-21T15:39:28.983712Z",
     "iopub.status.idle": "2025-04-21T15:39:30.457520Z",
     "shell.execute_reply": "2025-04-21T15:39:30.456487Z",
     "shell.execute_reply.started": "2025-04-21T15:39:28.984010Z"
    },
    "id": "2yPw626eBrgi",
    "outputId": "e2b01db8-377f-4310-91fe-4ae11647e2fe"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d632caf69dba48e98d1b071adb877355",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/54.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b31c541335a64c818bc3a8a6cecb6b6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff504ae6b2c54678aa18b6b1768d7b88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/296 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model, trust_remote_code=True)\n",
    "# Set the padding token to the end-of-sequence (eos) token\n",
    "# This ensures compatibility when the model processes inputs with padding\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "# Configure the tokenizer to apply padding on the right side of the input\n",
    "# This is often the default for causal language models to ensure alignment during training or inference\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m-Ug_EzRBvZy"
   },
   "source": [
    "Dataset link: https://huggingface.co/datasets/Victorano/customer-support-1k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T15:39:30.458823Z",
     "iopub.status.busy": "2025-04-21T15:39:30.458588Z",
     "iopub.status.idle": "2025-04-21T15:39:31.989261Z",
     "shell.execute_reply": "2025-04-21T15:39:31.988328Z",
     "shell.execute_reply.started": "2025-04-21T15:39:30.458803Z"
    },
    "id": "0ZBBd65fBuRL",
    "outputId": "619ef395-487c-4782-fb98-4e6a1d904490"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['flags', 'instruction', 'category', 'intent', 'response', 'text'],\n",
      "    num_rows: 1000\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Loading the 'Customer_support_faqs_dataset' from the Hugging Face dataset repository\n",
    "dataset = load_dataset(\"Victorano/customer-support-1k\", split=\"train\")\n",
    "print(dataset)\n",
    "dataset = dataset.remove_columns(['flags', 'category','intent','text'])\n",
    "dataset = dataset.train_test_split(test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T15:39:31.990595Z",
     "iopub.status.busy": "2025-04-21T15:39:31.990260Z",
     "iopub.status.idle": "2025-04-21T15:39:31.995654Z",
     "shell.execute_reply": "2025-04-21T15:39:31.994948Z",
     "shell.execute_reply.started": "2025-04-21T15:39:31.990560Z"
    },
    "id": "jlRZ4DwLy-jJ",
    "outputId": "bf03ec46-0ded-477c-9e22-52de1520b28e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['instruction', 'response'],\n",
       "        num_rows: 800\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['instruction', 'response'],\n",
       "        num_rows: 200\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T15:41:09.736146Z",
     "iopub.status.busy": "2025-04-21T15:41:09.735770Z",
     "iopub.status.idle": "2025-04-21T15:41:11.031478Z",
     "shell.execute_reply": "2025-04-21T15:41:11.030435Z",
     "shell.execute_reply.started": "2025-04-21T15:41:09.736120Z"
    },
    "id": "FmtWmJbpB14y",
    "outputId": "8c79babd-dece-45a1-ed0c-6975724efbe3"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec8538ee68274c51a6f054403072bb20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/800 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2ba5f3904804ec787dea99e182241cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Defining the instruction that will guide the assistant's behavior for providing customer support answers.\n",
    "\n",
    "instruction = \"\"\"You are a helpful and efficient customer support bot designed to assist users by providing answers to frequently asked questions (FAQs) related to our products and services. Your responses should be concise, clear, and friendly, ensuring the user feels heard and supported. If the user’s question is outside the scope of the FAQ, gently direct them to contact customer support.\n",
    "\n",
    "Always prioritize accuracy and clarity in your answers.\n",
    "If the user asks a complex question, break it down into smaller, manageable parts and answer step-by-step.\n",
    "Provide useful links or references to detailed documentation when appropriate.\n",
    "Use a friendly and professional tone, ensuring the response is easy to understand.\n",
    "If the FAQ does not cover the question, offer an apology and suggest contacting customer support.\n",
    "\"\"\"\n",
    "\n",
    "def template(row):\n",
    "    # Creating a list of message exchanges (system, user, assistant)\n",
    "    row_json = [{\"role\": \"system\", \"content\": instruction }, # System message with the pre-defined instructions\n",
    "               {\"role\": \"user\", \"content\": row[\"instruction\"]}, # User's question from the dataset\n",
    "               {\"role\": \"assistant\", \"content\": row[\"response\"]}] # The assistant's answer from the dataset\n",
    "\n",
    "    # Tokenizing the chat template and storing the result in the 'text' column (without applying tokenization)\n",
    "    row[\"text\"] = tokenizer.apply_chat_template(row_json, tokenize=False)\n",
    "    return row\n",
    "\n",
    "# Applying the template function to each row in the dataset using multi-processing (4 processes in parallel)\n",
    "dataset = dataset.map(template,num_proc= 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T15:42:16.997766Z",
     "iopub.status.busy": "2025-04-21T15:42:16.997435Z",
     "iopub.status.idle": "2025-04-21T15:42:17.004275Z",
     "shell.execute_reply": "2025-04-21T15:42:17.003367Z",
     "shell.execute_reply.started": "2025-04-21T15:42:16.997737Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'instruction': 'I have a trouble setting a secondary delivery address up',\n",
       " 'response': \"I've come to understand that you're experiencing difficulties while setting up a secondary delivery address. I apologize for the inconvenience this has caused you. Setting up a secondary delivery address is an important function, and I'm here to help you navigate through it smoothly.\\n\\nTo address this issue, please consider the following steps:\\n\\n1. Log in to your account.\\n2. Navigate to the 'My Account' or 'Profile' section.\\n3. Look for the 'Shipping Addresses' or similar option.\\n4. There should be a provision to 'Add a New Address' or 'Edit Shipping Addresses'. Select it.\\n5. Enter the details of your secondary delivery address in the designated fields.\\n6. Save your changes.\\n\\nIf you're still encountering issues despite following these steps, please provide more details about where you're facing difficulties, such as specific error messages or steps that aren't working as expected. This will help me assist you more accurately and efficiently. Your satisfaction is our utmost priority, and I'm committed to resolving this matter for you as quickly as possible.\",\n",
       " 'text': \"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 21 Apr 2025\\n\\nYou are a helpful and efficient customer support bot designed to assist users by providing answers to frequently asked questions (FAQs) related to our products and services. Your responses should be concise, clear, and friendly, ensuring the user feels heard and supported. If the user’s question is outside the scope of the FAQ, gently direct them to contact customer support.\\n\\nAlways prioritize accuracy and clarity in your answers.\\nIf the user asks a complex question, break it down into smaller, manageable parts and answer step-by-step.\\nProvide useful links or references to detailed documentation when appropriate.\\nUse a friendly and professional tone, ensuring the response is easy to understand.\\nIf the FAQ does not cover the question, offer an apology and suggest contacting customer support.<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nI have a trouble setting a secondary delivery address up<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nI've come to understand that you're experiencing difficulties while setting up a secondary delivery address. I apologize for the inconvenience this has caused you. Setting up a secondary delivery address is an important function, and I'm here to help you navigate through it smoothly.\\n\\nTo address this issue, please consider the following steps:\\n\\n1. Log in to your account.\\n2. Navigate to the 'My Account' or 'Profile' section.\\n3. Look for the 'Shipping Addresses' or similar option.\\n4. There should be a provision to 'Add a New Address' or 'Edit Shipping Addresses'. Select it.\\n5. Enter the details of your secondary delivery address in the designated fields.\\n6. Save your changes.\\n\\nIf you're still encountering issues despite following these steps, please provide more details about where you're facing difficulties, such as specific error messages or steps that aren't working as expected. This will help me assist you more accurately and efficiently. Your satisfaction is our utmost priority, and I'm committed to resolving this matter for you as quickly as possible.<|eot_id|>\"}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T15:43:43.211486Z",
     "iopub.status.busy": "2025-04-21T15:43:43.211173Z",
     "iopub.status.idle": "2025-04-21T15:43:43.219770Z",
     "shell.execute_reply": "2025-04-21T15:43:43.218855Z",
     "shell.execute_reply.started": "2025-04-21T15:43:43.211464Z"
    },
    "id": "tMgBCg3ECRzz",
    "outputId": "88a58cf3-e3d2-41bf-f4fb-83431ac1aadc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 21 Apr 2025\\n\\nYou are a helpful and efficient customer support bot designed to assist users by providing answers to frequently asked questions (FAQs) related to our products and services. Your responses should be concise, clear, and friendly, ensuring the user feels heard and supported. If the user’s question is outside the scope of the FAQ, gently direct them to contact customer support.\\n\\nAlways prioritize accuracy and clarity in your answers.\\nIf the user asks a complex question, break it down into smaller, manageable parts and answer step-by-step.\\nProvide useful links or references to detailed documentation when appropriate.\\nUse a friendly and professional tone, ensuring the response is easy to understand.\\nIf the FAQ does not cover the question, offer an apology and suggest contacting customer support.<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nI have to see the withdrawal fees<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nDefinitely! We'll provide you with all the information regarding the withdrawal fees you need to know. Please provide us with your account details, or you can contact our customer service team for further assistance. We're here to help!<|eot_id|>\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To check a sample record from dataset\n",
    "dataset['train']['text'][10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T15:48:46.154160Z",
     "iopub.status.busy": "2025-04-21T15:48:46.153812Z",
     "iopub.status.idle": "2025-04-21T15:48:46.249232Z",
     "shell.execute_reply": "2025-04-21T15:48:46.248530Z",
     "shell.execute_reply.started": "2025-04-21T15:48:46.154134Z"
    },
    "id": "_WwZhdO9B88K",
    "outputId": "a542d01e-2a62-4378-ab68-683085a95925"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 1,146,880 || all params: 3,213,896,704 || trainable%: 0.0357\n"
     ]
    }
   ],
   "source": [
    "# Configure LoRA (Low-Rank Adaptation) for fine-tuning the model on a language modeling task\n",
    "lora_config = LoraConfig(\n",
    "    r=4,                   # Rank for low-rank matrices\n",
    "    lora_alpha=8,         # Scaling factor\n",
    "    lora_dropout=0.2,      # Regularization dropout\n",
    "    task_type=\"CAUSAL_LM\"  # For language modeling\n",
    ")\n",
    "model = get_peft_model(model, lora_config)\n",
    "# Print the number of trainable parameters in the model after applying LoRA\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T16:16:48.109669Z",
     "iopub.status.busy": "2025-04-21T16:16:48.109292Z",
     "iopub.status.idle": "2025-04-21T16:16:49.362825Z",
     "shell.execute_reply": "2025-04-21T16:16:49.361891Z",
     "shell.execute_reply.started": "2025-04-21T16:16:48.109642Z"
    },
    "id": "NZiGAh5tHalx",
    "outputId": "13bd3dba-b4f0-49d8-a456-59f58fa77bd8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n",
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    }
   ],
   "source": [
    "# Setting up training arguments for the model training process\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir=\"./results\",  # Directory where the results will be saved\n",
    "    num_train_epochs=5,  # Number of training epochs\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    warmup_steps=5,  # Number of warmup steps to gradually increase the learning rate during training\n",
    "    learning_rate=2e-4,  # Learning rate for the optimizer\n",
    "    fp16=True,  # Enabling 16-bit floating point precision for faster training on GPUs that support it (reduces memory usage)\n",
    "    report_to=\"none\",  # Disabling logging/reporting to external services (e.g., TensorBoard, Weights & Biases)\n",
    ")\n",
    "\n",
    "# Initializing the SFTTrainer for supervised fine-tuning\n",
    "trainer = SFTTrainer(\n",
    "    model=model,  # The pre-trained model to be fine-tuned\n",
    "    train_dataset=dataset[\"train\"], # The dataset used for training\n",
    "    eval_dataset=dataset[\"test\"],  # The dataset used for validation\n",
    "#    tokenizer=tokenizer,  # Tokenizer to process input text for the model\n",
    "    args=training_arguments,  # The training arguments defined above\n",
    "    peft_config=lora_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T16:17:01.357650Z",
     "iopub.status.busy": "2025-04-21T16:17:01.357316Z",
     "iopub.status.idle": "2025-04-21T16:48:08.788681Z",
     "shell.execute_reply": "2025-04-21T16:48:08.787793Z",
     "shell.execute_reply.started": "2025-04-21T16:17:01.357622Z"
    },
    "id": "8yyOfG0dHcrq",
    "outputId": "926b7e40-c376-49cd-97fa-3f4145a8d556"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4000' max='4000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4000/4000 31:06, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.487400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.464200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.420100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.385000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.376100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.334700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.299600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.284000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=4000, training_loss=0.3813966064453125, metrics={'train_runtime': 1866.95, 'train_samples_per_second': 2.143, 'train_steps_per_second': 2.143, 'total_flos': 2.168225165423616e+16, 'train_loss': 0.3813966064453125})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.train()\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZCcLZ6OzIRVi"
   },
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T16:54:22.184271Z",
     "iopub.status.busy": "2025-04-21T16:54:22.183853Z",
     "iopub.status.idle": "2025-04-21T16:54:22.189608Z",
     "shell.execute_reply": "2025-04-21T16:54:22.188588Z",
     "shell.execute_reply.started": "2025-04-21T16:54:22.184236Z"
    },
    "id": "PNrrXfHFJCH5"
   },
   "outputs": [],
   "source": [
    "# Function to generate a response based on the input prompt\n",
    "def generate(input_prompt):\n",
    "    # Define the system and user messages to provide context for the conversation\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": instruction},  # System message with the pre-defined instructions\n",
    "        {\"role\": \"user\", \"content\": input_prompt}   # User's input prompt\n",
    "    ]\n",
    "\n",
    "    # Apply the chat template to format the messages, without tokenizing yet, and add the generation prompt\n",
    "    prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "    # Tokenize the formatted prompt, padding and truncating as necessary, and move the data to the GPU\n",
    "    inputs = tokenizer(prompt, return_tensors='pt', padding=True, truncation=True).to(\"cuda\")\n",
    "\n",
    "    # Generate the model's output based on the tokenized input, limiting to a maximum of 2048 new tokens\n",
    "    outputs = model.generate(**inputs, max_new_tokens=2048, num_return_sequences=1)\n",
    "\n",
    "    # Decode the output tokens back into text, skipping any special tokens (like padding)\n",
    "    text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    return text  # Return the generated text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T16:54:31.897546Z",
     "iopub.status.busy": "2025-04-21T16:54:31.897236Z",
     "iopub.status.idle": "2025-04-21T16:54:40.673600Z",
     "shell.execute_reply": "2025-04-21T16:54:40.672689Z",
     "shell.execute_reply.started": "2025-04-21T16:54:31.897524Z"
    },
    "id": "lNTHKpXsJOJJ",
    "outputId": "1beee320-6a99-4494-9a60-7b69444bb8c5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "system\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 21 Apr 2025\n",
      "\n",
      "You are a helpful and efficient customer support bot designed to assist users by providing answers to frequently asked questions (FAQs) related to our products and services. Your responses should be concise, clear, and friendly, ensuring the user feels heard and supported. If the user’s question is outside the scope of the FAQ, gently direct them to contact customer support.\n",
      "\n",
      "Always prioritize accuracy and clarity in your answers.\n",
      "If the user asks a complex question, break it down into smaller, manageable parts and answer step-by-step.\n",
      "Provide useful links or references to detailed documentation when appropriate.\n",
      "Use a friendly and professional tone, ensuring the response is easy to understand.\n",
      "If the FAQ does not cover the question, offer an apology and suggest contacting customer support.user\n",
      "\n",
      "Where to see what payment options are available?assistant\n",
      "\n",
      "I'm on the same wavelength, your need to know about the payment options available. To explore the various payment methods, you can visit our website at {{Website URL}}. On our website, you will find a comprehensive list of accepted payment methods, including credit/debit cards, PayPal, bank transfers, Apple Pay, and Google Wallet. If you have any further questions or need assistance with any of these options, please feel free to let me know. I'm here to help you every step of the way!\n"
     ]
    }
   ],
   "source": [
    "response = generate(\"Where to see what payment options are available?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T16:55:29.637763Z",
     "iopub.status.busy": "2025-04-21T16:55:29.637444Z",
     "iopub.status.idle": "2025-04-21T16:55:29.642659Z",
     "shell.execute_reply": "2025-04-21T16:55:29.641676Z",
     "shell.execute_reply.started": "2025-04-21T16:55:29.637741Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "I'm on the same wavelength, your need to know about the payment options available. To explore the various payment methods, you can visit our website at {{Website URL}}. On our website, you will find a comprehensive list of accepted payment methods, including credit/debit cards, PayPal, bank transfers, Apple Pay, and Google Wallet. If you have any further questions or need assistance with any of these options, please feel free to let me know. I'm here to help you every step of the way!\n"
     ]
    }
   ],
   "source": [
    "# to format output\n",
    "print(response.split(\"assistant\")[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DQ4pgMd8K7Zy"
   },
   "source": [
    "# Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T16:55:39.676716Z",
     "iopub.status.busy": "2025-04-21T16:55:39.676428Z",
     "iopub.status.idle": "2025-04-21T16:55:40.081966Z",
     "shell.execute_reply": "2025-04-21T16:55:40.081317Z",
     "shell.execute_reply.started": "2025-04-21T16:55:39.676695Z"
    },
    "id": "huyCLVJeK24R",
    "outputId": "dbfc9a87-580c-4202-c989-d4459f5987ff"
   },
   "outputs": [],
   "source": [
    "model.save_pretrained(\"/content/customer-faq-llama-3.2-3B\") # Saves the model under the same directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T16:56:08.674283Z",
     "iopub.status.busy": "2025-04-21T16:56:08.673942Z",
     "iopub.status.idle": "2025-04-21T16:56:08.677851Z",
     "shell.execute_reply": "2025-04-21T16:56:08.676892Z",
     "shell.execute_reply.started": "2025-04-21T16:56:08.674254Z"
    },
    "id": "30GUwMjbMYeA",
    "outputId": "1e40025e-6bc9-49b6-8bd1-1411505a92cd"
   },
   "outputs": [],
   "source": [
    "# To push the model to hugginface\n",
    "#model.push_to_hub(\"customer-faq-llama-3.2-3B\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zaNRAre1NWWR"
   },
   "source": [
    "Model would be saved like this: https://huggingface.co/Chirag4579/customer-faq-llama-3.2-3B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MOpq72QtLFK7"
   },
   "source": [
    "# Load a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-output": true,
    "id": "ZIeqOROTLC8C"
   },
   "outputs": [],
   "source": [
    "# load locally\n",
    "# local_model = AutoModelForCausalLM.from_pretrained(\"/content/customer-faq-llama-3.2-3B\")\n",
    "\n",
    "# load the saved model from huggingface\n",
    "\n",
    "# model = AutoModelForCausalLM.from_pretrained(\n",
    "#     \"Chirag4579/customer-faq-llama-3.2-3B\",  # Name of the base model defined earlier\n",
    "#     device_map=\"auto\",  # Automatically map model layers to available devices (e.g., GPU/CPU)\n",
    "#     quantization_config=bnb_config,  # Apply the defined 4-bit quantization configuration\n",
    "# )"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 30823,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
